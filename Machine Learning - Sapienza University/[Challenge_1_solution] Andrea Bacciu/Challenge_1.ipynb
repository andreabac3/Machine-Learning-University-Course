{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classify'em all!</h1>\n",
    "<img src=\"figura.svg\"/> \n",
    "<h2>Machine Learning 2019/2020 - Challenge 1 - 11 november 2019</h2>\n",
    "\n",
    "\n",
    "<h3>Rules:</h3>\n",
    "<ul>\n",
    "    <li> - This year the results of the two challenges will counts 10% and 15% of your final score.</li>\n",
    "    <li> - If you work with a group of colleges (max 3 students), please remember your solution must be \"your solution\", hence provide your individual answers/arguments/opinions/critics;</li>\n",
    "    <li> - Students of the same group can share ONLY the code.</li>\n",
    "    <li> - Mail your solution (a jupyter notebook, or a collection of source files and txt files or PDFs) only to stefano.faralli@unitelmasapienza.it before the 11:59 PM of the 12 November 2019 (Rome Berlin time);</li>\n",
    "    <li> - The subject of your email must be: \"[Challenge_1_solution] NAME - SURNAME - MATRICOLA.\"</li>\n",
    "    <li> - Double check the subject of your email and the attachments;</li>\n",
    "    <li> - In case you want to compress the attachments, USE ONLY STANDARD ZIP compression;</li>\n",
    "    <li> - The physical attendance to the lab is not mandatory, you can work from a remote place by following the identical rules.</li>\n",
    "    <li> - Your solution might be considered as the \"copy\" of others solutions, in that specific case the assigned score will be the result of the proposed solution divided by the number of \"too much similar\" solutions. If you share the code with the components of your \"group\" please comment the code and answer the questions by yourself. </li>\n",
    " </ul>\n",
    "<h3>Instructions:</h3>\n",
    "<ul>\n",
    " <li> - Monday 11 Novemeber, I will share the necessary key to decrypt the content of the provided challenge material  Zip (I will also send a message on the Google group).</li>\n",
    " <li> - To decrypt the content of the zip you will use the \"Challenghe_1_setup.ipynb\" and the above key</li>\n",
    " <li> - Then read carefully all the part of the jupyter notebook \"decrypted/Challenghe_1.ipynb\" and fill all mandatory fields.</li>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Identification (Mandatory)</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identification\n",
    "# assign the following string values\n",
    "yourNameSurname='andreabac3'\n",
    "yourMatricolaNumber='andreabac3'\n",
    "yourStudentEMAIL='andreabac3'\n",
    "yourColleguesNameSurnames=['andreabac3','andreabac3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Part 1: Dataset  (5/30 p.ts)</H1>\n",
    "<p> In the folder you can find the dataset \"dataset_noisy.csv\"</p>\n",
    "<ol>\n",
    "    <li>Spot and fix the errors (2/30 p.ts);<br> <b>PLEASE NOTE THE DATASET IS DIFFERENT FROM THE ONE YOU MAY FIND ELSEWHERE ON THE WEB</b></li>\n",
    "    <li>Write the python code to save the corresponding \"DataFrame\" into file named \"Pokemon_clean.csv\" (respecting the original format); (1/30 p.ts)<br><b>PLEASE REMEBRER TO SEND the saved file by email</b></li>\n",
    "    <li>Write the python code to generate the corresponding \"DataFrame\"; (1/30 p.ts) </li>\n",
    "    <li>Show the DataFrame content and print the shape; (1/30 p.ts) </li>\n",
    " </ol>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(946, 13)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "PATH_FILE = \"./Pokemon_clean.csv\"\n",
    "df = pd.read_csv(PATH_FILE, delimiter = \";\", error_bad_lines=False)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Part 2: Data Analysis (5/30 p.ts)</H1>\n",
    "<p> Using the above created dataset write the python code to:</li>\n",
    "    <li>Count the number of Non-Legendary and Legendary Pokemon; (1/30 p.ts)</li>\n",
    "    <li>Count the number of pokemons of the same \"Type 1\" category; (2/30 p.ts)</li>\n",
    "    <li>Count the average HP for the pokemons of the same \"Type 1\" category; (2/30 p.ts)</li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Number',\n",
       " 'Name',\n",
       " 'Type1',\n",
       " 'Type2',\n",
       " 'Total',\n",
       " 'HP',\n",
       " 'Attack',\n",
       " 'Defense',\n",
       " 'SpecialAtk',\n",
       " 'SpecialDef',\n",
       " 'Speed',\n",
       " 'Generation',\n",
       " 'Legendary']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#write from that cell the code for Part 2 (insert additional cells below if needed) \n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    843\n",
      "True     103\n",
      "Name: Legendary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Legendary\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Water       124\n",
      "Normal      110\n",
      "Grass        86\n",
      "Psychic      85\n",
      "Bug          82\n",
      "Fire         59\n",
      "Electric     54\n",
      "Rock         54\n",
      "Poison       37\n",
      "Ghost        37\n",
      "Dragon       36\n",
      "Dark         34\n",
      "Steel        34\n",
      "Ground       34\n",
      "Fighting     33\n",
      "Ice          24\n",
      "Fairy        19\n",
      "Flying        4\n",
      "Name: Type1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Type1\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          HP\n",
      "Type1       \n",
      "Bug       59\n",
      "Dark      80\n",
      "Dragon    80\n",
      "Electric  63\n",
      "Fairy     72\n",
      "Fighting  72\n",
      "Fire      69\n",
      "Flying    70\n",
      "Ghost     64\n",
      "Grass     66\n",
      "Ground    74\n",
      "Ice       72\n",
      "Normal    77\n",
      "Poison    66\n",
      "Psychic   77\n",
      "Rock      67\n",
      "Steel     69\n",
      "Water     70\n"
     ]
    }
   ],
   "source": [
    "print(df[['Type1','HP']].groupby(['Type1']).mean().astype(int) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 3: Legendary Pokemon Classification (10/30 p.ts)</h1>\n",
    "Using the above created dataset:\n",
    "<ol>\n",
    "<li> write the python code to implement two different classifier (by means of different algorithms) to classify an unobserved pokemon into the \"Legendary\" and \"Non-Legendary\" categories. Split your dataset in order to have a 80% for training and a 20% for testing. Train and Test with the resulting dataset; (2.5 p.ts for each calssifier (max p.ts 5))</li>\n",
    " <li> discuss the selection of the parameters (if any) you empirically choose to instanciate each classifier; (2.5 p.ts for each calssifier (max p.ts 5))</li></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il seguente codice, divide in train e test il dataset, effetto feature selection andando ad eliminare le feature che non apportano un grande contributo ai fini della classificazione.\n",
    "la funzione print_accuracy chiama due funzioni della librearia sklearn, Ã¨ utilizzata come shortcut\n",
    "Ho fissato il seed del random_state a 0 per evitare di effettuare dei test con continue variazioni. Va successivamente rimosso per evitare di overfittare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(pred, label_test, name_clf):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"\\n\" + name_clf)\n",
    "    tn, fp, fn, tp =  confusion_matrix(label_test, pred).ravel()\n",
    "    precision=tp/(tp+fp)\n",
    "    print('Precision: ', precision)\n",
    "    recall=tp/(tp+fn)\n",
    "    print('Recall: ', recall)\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    print('Accuracy:', accuracy)\n",
    "    f1=2.0*((precision*recall)/(precision+recall))\n",
    "    print('F1-Score: ', f1)\n",
    "    TNR=tn/(tn+fp)\n",
    "    print('TNR: ', TNR)\n",
    "    FPR=fp/(fp+tn)\n",
    "    print('FPR: ',  FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_map_toCategorial(feature):\n",
    "    #usare solo se entrambi i tipi sono utilizzati altrimenti scegliere uno dei due\n",
    "    return [type1Categorial[feature[0]]] + [type2Categorial[feature[1]]] + feature[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Grass': 0, 'Fire': 1, 'Water': 2, 'Bug': 3, 'Normal': 4, 'Poison': 5, 'Electric': 6, 'Ground': 7, 'Fairy': 8, 'Fighting': 9, 'Psychic': 10, 'Rock': 11, 'Ghost': 12, 'Ice': 13, 'Dragon': 14, 'Dark': 15, 'Steel': 16, 'Flying': 17}\n"
     ]
    }
   ],
   "source": [
    "type1_string = df['Type1'].unique().tolist()\n",
    "type1Categorial = {}\n",
    "interval = range(len(type1_string))\n",
    "for x, y in zip(type1_string,interval ):\n",
    "    type1Categorial[x] = y\n",
    "print(type1Categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Poison': 0, nan: 1, 'Flying': 2, 'Dragon': 3, 'Ground': 4, 'Fairy': 5, 'Grass': 6, 'Fighting': 7, 'Psychic': 8, 'Steel': 9, 'Ice': 10, 'Rock': 11, 'Dark': 12, 'Water': 13, 'Electric': 14, 'Fire': 15, 'Ghost': 16, 'Bug': 17, 'Normal': 18, 'Fair': 19}\n"
     ]
    }
   ],
   "source": [
    "type2_string = df['Type2'].unique().tolist()\n",
    "type2Categorial = {}\n",
    "interval = range(len(type2_string))\n",
    "for x, y in zip(type2_string,interval):\n",
    "    type2Categorial[x] = y\n",
    "print(type2Categorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(feature_list):\n",
    "    names = []\n",
    "    feature = []\n",
    "    for elem in feature_list:\n",
    "        names.append(elem[0])\n",
    "        feature_list.append(elem[1:])\n",
    "    return names, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Total', 'HP', 'Attack', 'SpecialAtk', 'SpecialDef']\n",
      "['Legendary']\n",
      "['Tapu Lele', 570, 70, 85, 130, 115]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def print_accuracy(pred, labels_test):\n",
    "    print(accuracy_score(pred, labels_test))\n",
    "    target = [\"not Legendary\", \"Legendary\" ]\n",
    "    print(classification_report(y_true=labels_test, y_pred=pred, target_names=target))\n",
    "import numpy as np\n",
    "df1 = df.drop(['Number', 'Legendary', 'Type1', 'Type2', 'Generation', 'Speed', 'Defense'], axis=1)  \n",
    "print(df1.columns.tolist())\n",
    "df2 = df[['Legendary']]\n",
    "print(df2.columns.tolist())\n",
    "df2 = df2.values.tolist()\n",
    "df1 = df1.values.tolist()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "feature_train_dt_tmp, feature_test_dt_tmp, label_train_dt, label_test_dt = train_test_split(df1, df2, test_size=0.2, random_state=0)\n",
    "label_train = np.asarray(label_train_dt)\n",
    "label_test_dt = np.asarray(label_test_dt)\n",
    "print(feature_test_dt_tmp[0])\n",
    "feature_test_dt = []\n",
    "feature_train_dt = []\n",
    "for elem in feature_test_dt_tmp:\n",
    "    feature_test_dt.append(elem[1:])\n",
    "for elem in feature_train_dt_tmp:\n",
    "    feature_train_dt.append(elem[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho provato a trasformare le feature 'Type1' e Type2 in categorial feature ma non aiutano la classificazione\n",
    "#feature_train = list(map(func_map_toCategorial, feature_train))\n",
    "#feature_test = list(map(func_map_toCategorial, feature_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9894736842105263\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not Legendary       1.00      0.99      0.99       174\n",
      "    Legendary       0.89      1.00      0.94        16\n",
      "\n",
      "    micro avg       0.99      0.99      0.99       190\n",
      "    macro avg       0.94      0.99      0.97       190\n",
      " weighted avg       0.99      0.99      0.99       190\n",
      "\n",
      "\n",
      "Decision Tree C\n",
      "Precision:  0.8888888888888888\n",
      "Recall:  1.0\n",
      "Accuracy: 0.9894736842105263\n",
      "F1-Score:  0.9411764705882353\n",
      "TNR:  0.9885057471264368\n",
      "FPR:  0.011494252873563218\n"
     ]
    }
   ],
   "source": [
    "#write from that cell the code for Part 3 (insert additional cells below if needed) \n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(feature_train_dt, label_train_dt)\n",
    "pred = clf.predict(feature_test_dt)\n",
    "#train, val = train_test_split(train, test_size=0.2)\n",
    "print_accuracy(pred, label_test_dt)\n",
    "print_evaluation(pred, label_test_dt, 'Decision Tree C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'HP', 'Attack', 'SpecialAtk', 'SpecialDef', 'Speed']\n",
      "['Legendary']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def print_accuracy(pred, labels_test):\n",
    "    print(\"Accuracy: \", accuracy_score(pred, labels_test))\n",
    "    target = [\"not Legendary\", \"Legendary\" ]\n",
    "    print(classification_report(y_true=labels_test, y_pred=pred, target_names=target))\n",
    "import numpy as np\n",
    "df = None\n",
    "with open (PATH_FILE, \"r\") as file:\n",
    "        df = pd.read_csv(file, delimiter = \";\", error_bad_lines=False)\n",
    "df1 = df.drop(['Legendary', 'Type1','Type2', 'Number', 'Total', 'Generation',  'Defense'], axis=1)  \n",
    "print(df1.columns.tolist())\n",
    "df2 = df[['Legendary']]\n",
    "print(df2.columns.tolist())\n",
    "df2 = df2.values.tolist()\n",
    "df1 = df1.values.tolist()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "feature_train_tmp, feature_test_tmp, label_train, label_test = train_test_split(df1, df2, test_size=0.2, random_state=0)\n",
    "label_train = np.asarray(label_train)\n",
    "label_test = np.asarray(label_test)\n",
    "feature_test = []\n",
    "feature_train = []\n",
    "for elem in feature_train_tmp:\n",
    "    feature_train.append(elem[1:])\n",
    "for elem in feature_test_tmp:\n",
    "    feature_test.append(elem[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MULTI LAYER P\n",
      "Precision:  0.5\n",
      "Recall:  0.3125\n",
      "Accuracy: 0.9157894736842105\n",
      "F1-Score:  0.38461538461538464\n",
      "TNR:  0.9712643678160919\n",
      "FPR:  0.028735632183908046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLP = MLPClassifier(random_state=0)\n",
    "MLP.fit(feature_train, label_train)\n",
    "#print(label_train)\n",
    "pred_mlp = MLP.predict(feature_test)\n",
    "print_evaluation(pred_mlp, label_test, 'MULTI LAYER P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 4: Evaluation (10/30 p.ts)</h1>\n",
    "<p>Starting from the above created classifications provide:\n",
    "<ol>\n",
    "    <li> the number of TP, TN, FP, FN; (2 p.t for each calssifier (4 p.ts max))</li>\n",
    "    <li> the precision, recall, f1 and accuracy + (90% confidence interval); (1 p.t for each calssifier (2 p.ts max))</li>\n",
    "    <li> the full description of two wrongly classified pokemons (1 p.t for each calssifier (2 p.ts max))</li>\n",
    "    <li> A discussion about the different classification outcomes. Why you obtain different classification performances?(1 p.t for each calssifier (2 p.ts max))</li>\n",
    "</ol>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DECISION TREE\n",
      "Precision:  0.8888888888888888\n",
      "Recall:  1.0\n",
      "Accuracy: 0.9894736842105263\n",
      "F1-Score:  0.9411764705882353\n",
      "TNR:  0.9885057471264368\n",
      "FPR:  0.011494252873563218\n",
      "Accuracy:  0.9894736842105263\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not Legendary       1.00      0.99      0.99       174\n",
      "    Legendary       0.89      1.00      0.94        16\n",
      "\n",
      "    micro avg       0.99      0.99      0.99       190\n",
      "    macro avg       0.94      0.99      0.97       190\n",
      " weighted avg       0.99      0.99      0.99       190\n",
      "\n",
      "True Positive: 16 True Negative 172 False Positive 2 False Negative 0\n",
      "\n",
      " MULTI LAYER PERCEPTRON\n",
      "Accuracy:  0.9157894736842105\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "not Legendary       0.94      0.97      0.95       174\n",
      "    Legendary       0.50      0.31      0.38        16\n",
      "\n",
      "    micro avg       0.92      0.92      0.92       190\n",
      "    macro avg       0.72      0.64      0.67       190\n",
      " weighted avg       0.90      0.92      0.91       190\n",
      "\n",
      "\n",
      "MLP\n",
      "Precision:  0.5\n",
      "Recall:  0.3125\n",
      "Accuracy: 0.9157894736842105\n",
      "F1-Score:  0.38461538461538464\n",
      "TNR:  0.9712643678160919\n",
      "FPR:  0.028735632183908046\n",
      "True Positive: 16 True Negative 172 False Positive 2 False Negative 0\n"
     ]
    }
   ],
   "source": [
    "#write from that cell the code for Part 4 (insert additional cells below if needed) \n",
    "from sklearn.metrics import confusion_matrix\n",
    "print_evaluation(pred, label_test, \"DECISION TREE\")\n",
    "print_accuracy(pred, label_test)\n",
    "tn, fp, fn, tp =  confusion_matrix(label_test, pred).ravel()\n",
    "print(\"True Positive:\" , tp , \"True Negative\", tn, \"False Positive\", fp, \"False Negative\", fn)\n",
    "print(\"\\n MULTI LAYER PERCEPTRON\")\n",
    "print_accuracy(pred_mlp, label_test)\n",
    "print_evaluation(pred_mlp, label_test, \"MLP\")\n",
    "tn, fp, fn, tp =  confusion_matrix(label_test, pred).ravel()\n",
    "print(\"True Positive:\" , tp , \"True Negative\", tn, \"False Positive\", fp, \"False Negative\", fn)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon wrongly classified      Number  Name Type1   Type2  Total  HP  Attack  Defense  SpecialAtk  \\\n",
      "103      95  Onix  Rock  Ground    385  35      45      160          30   \n",
      "\n",
      "     SpecialDef  Speed  Generation  Legendary  \n",
      "103          45     70           1      False  \n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE\n",
    "for i in range(len(feature_test_dt_tmp)):\n",
    "    if pred[i] != label_test_dt[i]:\n",
    "        print(\"Pokemon wrongly classified\", df.loc[df['Number'] == feature_test[i][0]])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pokemon wrongly classified     Number        Name  Type1   Type2  Total  HP  Attack  Defense  SpecialAtk  \\\n",
      "76      70  Weepinbell  Grass  Poison    390  65      90       50          85   \n",
      "\n",
      "    SpecialDef  Speed  Generation  Legendary  \n",
      "76          45     55           1      False  \n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "for i in range(len(feature_test)):\n",
    "    if pred_mlp[i] != label_test[i]:\n",
    "        print(\"Pokemon wrongly classified\", df.loc[df['Number'] == feature_test[i][0]])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.9140276519112349 (+/- 0.01671473170774021 )\n"
     ]
    }
   ],
   "source": [
    "constant_z = 1.64\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(DecisionTreeClassifier(random_state=0), feature_train, label_train, cv=5)\n",
    "print(\"Acc: \", scores.mean(), \"(+/-\", scores.std() * constant_z, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.809748925293366 (+/- 0.15312535442672556 )\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(MLPClassifier(random_state=0), feature_train, label_train, cv=5)\n",
    "print(\"Acc: \", scores.mean(), \"(+/-\", scores.std() * constant_z, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 5: Bonus</h1>\n",
    "<ol>\n",
    "<li>Additional 1 point will be assigned to those solutions where the resulting F1 is better than the one the Teacher was able to obtain himself.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRID SEARCH -> DT\n",
      "Precision:  0.8\n",
      "Recall:  1.0\n",
      "Accuracy: 0.9789473684210527\n",
      "F1-Score:  0.888888888888889\n",
      "TNR:  0.9770114942528736\n",
      "FPR:  0.022988505747126436\n",
      "Best parameters set found on development set:\n",
      "{'max_features': 3, 'min_samples_leaf': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_parameters = [{'min_samples_leaf': [1, 2, 3,4], 'max_features': [2, 3, 4, 5]}]\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=0), tuned_parameters, cv=5, n_jobs=-1)\n",
    "clf.fit(feature_train_dt, label_train_dt)\n",
    "pred = clf.predict(feature_test_dt)\n",
    "print_evaluation(pred, label_test, \"GRID SEARCH -> DT\")\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRID -> MLP\n",
      "Precision:  0.35714285714285715\n",
      "Recall:  0.3125\n",
      "Accuracy: 0.8947368421052632\n",
      "F1-Score:  0.3333333333333333\n",
      "TNR:  0.9482758620689655\n",
      "FPR:  0.05172413793103448\n",
      "Best parameters set found on development set:\n",
      "{'activation': 'relu', 'hidden_layer_sizes': 3, 'solver': 'lbfgs'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/home/andrea/.local/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:916: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tuned_parameters = [{'activation': [\"identity\", \"logistic\", \"tanh\", \"relu\"], 'solver': ['lbfgs', 'sgd', 'adam'], 'hidden_layer_sizes':[10, 100, 3, 4, 50]}]\n",
    "clf = GridSearchCV(MLPClassifier(random_state=0), tuned_parameters, cv=5, n_jobs=-1)\n",
    "clf.fit(feature_train, label_train)\n",
    "pred = clf.predict(feature_test)\n",
    "print_evaluation(pred, label_test, \"GRID -> MLP\")\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
